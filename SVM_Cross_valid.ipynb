{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split #just for crossvalidation \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cvxopt\n",
    "import cvxopt.solvers\n",
    "from collections import Counter\n",
    "from itertools import combinations_with_replacement\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtr0 = pd.read_csv(\"Data/Xtr0.csv\", \" \", header=0)\n",
    "xtr1 = pd.read_csv(\"Data/Xtr1.csv\", \" \", header=0)\n",
    "xtr2 = pd.read_csv(\"Data/Xtr2.csv\", \" \", header=0)\n",
    "xtrain_temp = np.append(np.append(xtr0, xtr1), xtr2)\n",
    "xtrain = np.array(xtrain_temp)\n",
    "\n",
    "xte0 = pd.read_csv(\"Data/Xte0.csv\", \" \", header=0)\n",
    "xte1 = pd.read_csv(\"Data/Xte1.csv\", \" \", header=0)\n",
    "xte2 = pd.read_csv(\"Data/Xte2.csv\", \" \", header=0)\n",
    "xtest_temp = np.append(np.append(xte0, xte1), xte2)\n",
    "xtest = np.array(xtest_temp)\n",
    "\n",
    "ytr0 = pd.read_csv(\"Data/Ytr0.csv\", index_col=0, header=0)\n",
    "ytr1 = pd.read_csv(\"Data/Ytr1.csv\", index_col=0, header=0)\n",
    "ytr2 = pd.read_csv(\"Data/Ytr2.csv\", index_col=0, header=0)\n",
    "ytrain_temp = np.append(np.append(ytr0, ytr1), ytr2)\n",
    "ytrain = np.array(ytrain_temp)\n",
    "ytrain[ytrain[:] == 0] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparing features: kmers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All possible substrings of size 5\n",
    "def create_subsequences(lenght):\n",
    "    p = ['A','C','G','T','C','G','T','A','G','T','A','C','T','A','C','G']\n",
    "    subseq = []\n",
    "    for i in combinations_with_replacement(p, lenght):\n",
    "        subseq.append(list(i))\n",
    "    subseq = np.asarray(subseq)\n",
    "    subseq= np.unique(subseq, axis = 0)    \n",
    "    subseq =[\"\".join(j) for j in subseq[:,:].astype(str)]\n",
    "    return subseq\n",
    "\n",
    "def create_kmers(x, y, test, k):\n",
    "    subseq = create_subsequences(k)\n",
    "    features = np.zeros((len(x), len(subseq)))   #To store the occurence of each string\n",
    "    for i in range(0,len(x)):\n",
    "        s = x[i]\n",
    "        c = [ s[j:j+k] for j in range(len(s)-k+1) ]\n",
    "        counter = Counter(c)\n",
    "        j=0\n",
    "        for m in subseq:\n",
    "            features[i][j] = counter[m]\n",
    "            j=j+1\n",
    "            \n",
    "    features0 = features[y[:] == 0]\n",
    "    features0_sum = features0.sum(axis=0)\n",
    "    index0 = np.argpartition(features0_sum, 0)[:]\n",
    "    \n",
    "    features1 = features[y[:] == 1]\n",
    "    features1_sum = features1.sum(axis=0)\n",
    "    print(features1_sum)\n",
    "    index1 = np.argpartition(features1_sum, 0)[:]\n",
    "    \n",
    "    index = np.append(index0,index1)\n",
    "    features_train = features[:,index]\n",
    "    features_train = features_train / np.max(np.abs(features_train),axis=0)\n",
    "\n",
    "    if test.size != 0:\n",
    "        features_test = create_test_feature(test,subseq,index, k)\n",
    "        return features_train , features_test\n",
    "    else:\n",
    "        return features_train    \n",
    "    \n",
    "def create_test_feature(testdata, subsequences, ind, k):\n",
    "    features_test = np.zeros((len(testdata), len(subsequences)))\n",
    "    for i in range(0,len(testdata)):\n",
    "        s = testdata[i]\n",
    "        c = [ s[j:j+k] for j in range(len(s)-k+1) ]\n",
    "        counter = Counter(c)\n",
    "        j = 0\n",
    "        for m in subsequences:\n",
    "            features_test[i][j] = counter[m]\n",
    "            j = j+1\n",
    "    features_test = features_test[:,[ind]]\n",
    "    features_test = features_test / np.max(np.abs(features_test),axis=0)\n",
    "\n",
    "    return features_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Different non linear kernels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_kernel(x, y, p = 3):\n",
    "    return (1 + np.dot(x, y)) ** p\n",
    "\n",
    "def rbf_kernel(x, y, sigma = 3):\n",
    "    return np.exp(-np.linalg.norm(x-y)**2 / (2 * (sigma ** 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM(object):\n",
    "    def __init__(self, kernel = polynomial_kernel, C = None):\n",
    "        self.kernel = kernel\n",
    "        self.C = C\n",
    "        if self.C is not None: self.C = float(self.C)\n",
    "            \n",
    "    def fit_svm(self, x, y):\n",
    "        num_obs, num_features = x.shape\n",
    "        #Gram matrix\n",
    "        Gram = np.zeros((num_obs, num_obs))\n",
    "        for i in range(num_obs):\n",
    "            if (i%100 == 0):\n",
    "               print(i, \"/\", num_obs)\n",
    "            for j in range(num_obs):\n",
    "                Gram[i,j] = self.kernel(x[i], x[j])  \n",
    "                \n",
    "        #Components for quadratic program problem        \n",
    "        P = cvxopt.matrix(np.outer(y,y) * Gram)\n",
    "        q = cvxopt.matrix(-np.ones((num_obs, 1)))\n",
    "        A = cvxopt.matrix(y, (1,num_obs), 'd')\n",
    "        b = cvxopt.matrix(np.zeros(1))\n",
    "        diag = np.eye(num_obs)\n",
    "        G = cvxopt.matrix(np.vstack((diag, diag * -1)))\n",
    "        h = cvxopt.matrix(np.hstack((np.zeros(num_obs), np.ones(num_obs) * self.C)))\n",
    "        \n",
    "        #Solving quadratic progam problem\n",
    "        sol = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "        alphas = np.ravel(sol['x'])\n",
    "        \n",
    "        #Support vectors have non zero lagrange multipliers, cut off at 1e-5\n",
    "        sup_vec = alphas > 1e-6\n",
    "        ind = np.arange(len(alphas))[sup_vec]\n",
    "        \n",
    "        #Creating support vectors\n",
    "        self.alphas = alphas[sup_vec]\n",
    "        self.sup_vec = x[sup_vec]\n",
    "        self.sup_vec_y = y[sup_vec]\n",
    "        \n",
    "        #Fitting support vectors with the intercept\n",
    "        self.b = 0\n",
    "        for i in range(len(self.alphas)):\n",
    "            self.b += self.sup_vec_y[i]\n",
    "            self.b -= np.sum(self.alphas * self.sup_vec_y * Gram[ind[i],sup_vec])\n",
    "        self.b /= len(self.alphas)\n",
    "        print(self.b)\n",
    "        \n",
    "        #Weight for non linear kernel(polynomial or rbf)\n",
    "        self.w = None  \n",
    "            \n",
    "    #Predict the sign\n",
    "    def predict(self, X):\n",
    "        y_pred = np.zeros(len(X))\n",
    "        for i in range(len(X)):\n",
    "            s = 0\n",
    "            for alphas, sup_vec_y, sup_vec in zip(self.alphas, self.sup_vec_y, self.sup_vec):\n",
    "                s += alphas * sup_vec_y * self.kernel(X[i], sup_vec)\n",
    "            y_pred[i] = s\n",
    "        return np.sign(y_pred + self.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 710.  328.  347.  490.  469.  298.   86.  286.  333.  292.  326.  266.\n",
      "  332.  212.  459.  314.  365.  358.  444.  290.  348.  288.  126.  304.\n",
      "   51.  108.  100.   97.  167.  235.  338.  220.  388.  213.  339.  233.\n",
      "  402.  350.  107.  275.  376.  321.  366.  242.  200.  168.  367.  243.\n",
      "  287.  189.  212.  243.  316.  227.   55.  251.  382.  217.  297.  309.\n",
      "  222.  218.  248.  323.  363.  258.  321.  300. 1523.  406.  328.  336.\n",
      "  485.  407.  486.  267.  304.  240.  824.  315.  272.  433.  387.  304.\n",
      "  441.  332.  134.  295.   74.  125.  117.   66.  164.  392.  473.  230.\n",
      "   46.   49.   86.   46.  267.  180.  109.  107.  102.  123.  114.   93.\n",
      "   84.  112.  394.   78.  175.  117.  146.  147.  329.  322.   66.  279.\n",
      "  272.  370.  361.  324.  153.  237.  282.  259.  444.  265.  386.  311.\n",
      "  394.  276.  133.  256.  444.  358.  481.  272.  230.  168.  335.  243.\n",
      "  489.  353.  542.  311.  533.  455.  172.  454.  123.  139.  150.  124.\n",
      "  183.  389.  537.  259.  478.  288.  549.  302.  587.  371.  206.  450.\n",
      "  473.  420.  428.  274.  188.  243.  432.  296.  208.  126.  198.  140.\n",
      "  303.  211.   50.  223.  417.  251.  362.  357.  193.  207.  345.  289.\n",
      "  267.  137.  186.  207.  311.  127.   43.  169.  156.  195.  145.  158.\n",
      "  165.  129.  229.  210.  178.  314.  303.  261.  265.  252.   58.  285.\n",
      "   42.   85.   41.   49.  144.  276.  322.  245.  328.  259.  335.  254.\n",
      "  703.  288.  100.  322.  322.  421.  372.  315.  231.  242.  674.  304.\n",
      "  168.  167.  177.  226.  245.  222.   42.  275.  165.  406.  278.  266.\n",
      "  229.  337.  269.  455.  437.  268.  356.  253.  502.  397.  135.  294.\n",
      "  344.  310.  431.  275.  243.  305.  306.  250.  459. 1602.  618.  532.\n",
      "  501.  150.  490.  157.  158.  383.  544.  326.  426.  358.  585.  342.\n",
      "  614.  666.  211.  539.  595.  627.  604.  450.  192.  254.  501.  329.\n",
      "  196.  224.  230.  222.  399.  342.   74.  370.  631.  211.  250.  335.\n",
      "  404.  326.  275.  394.  294.  608.  567.  304.  364.  382.  694.  815.\n",
      "  330.  200.  350.  577.  308.  365.  565.  746.  376.  566.  370.  265.\n",
      "  469.   82.  288.  289.  154.  224.  478.  614.  355.   74.   90.  138.\n",
      "   39.  156.  349.  194.  164.  139.  249.  271.  113.   46.  137.  179.\n",
      "   85.  185.  148.  283.  160.  428.  691.  205.  443.  371.  594.  781.\n",
      "  472.  197.  367.  347.  332.   91.   59.   66.   52.   93.  102.   56.\n",
      "   62.   98.   91.  173.   73.   22.   32.   76.   71.  179.  278.  196.\n",
      "   95.  261.  267.  183.  228.   61.  244.  172.  103.   67.  167.  206.\n",
      "  108.   91.   88.  172.   68.  139.  255.  171.  184.  183.  184.  270.\n",
      "  129.   61.  100.  165.  104.   87.   43.   65.   43.  166.  134.   68.\n",
      "  115.  126.  241.  290.  244.   45.   71.  120.   82.  227.  138.  195.\n",
      "  170.  177.  166.   50.  183.  218.  206.  260.  156.   94.  106.  212.\n",
      "  184.  308.  461.  487.  332.  549.  662.  174.  554.   81.  134.  184.\n",
      "  109.  201.  436.  593.  338.  320.  289.  487.  280.  566.  616.  184.\n",
      "  518.  476.  596.  686.  406.  228.  452.  600.  447.  184.  178.  190.\n",
      "  157.  293.  475.   76.  378.  282.  315.  377.  286.  204.  354.  330.\n",
      "  346.  406.  362.  342.  357.  267.  197.   86.  216.  303.  348.  352.\n",
      "  252.  193.  180.  247.  201.  212.  320.  369.  272.  262.   57.  286.\n",
      "  128.  128.  231.  267.  192.  396.  351.  397.  271.  381.  411.  142.\n",
      "  325.  448.  486.  462.  310.  146.  243.  308.  287.  186.  125.  120.\n",
      "  114.  170.  176.   50.  154.  312.  140.  171.  297.  220.  333.  613.\n",
      "  345.  358.  919.  300.  194.  300.  480.  534.  597.  368.  211.  270.\n",
      "  510.  257.  400.  492.  619.  397.  523.  501.  226.  396.  122.  301.\n",
      "  235.  136.  192.  465.  586.  286.   87.  123.  136.   78.  227.  261.\n",
      "  205.  157.  133.  259.  270.  140.   71.  151.  241.   86.  197.  168.\n",
      "  212.  145.  426.  454.  117.  377.  405.  516.  617.  428.  180.  302.\n",
      "  308.  289.  482.  233.  486.  222.  343.  250.  135.  220.  474.  414.\n",
      "  608.  320.  124.  168.  324.  220.  506.  360.  671.  279.  515.  430.\n",
      "  267.  338.  157.  249.  341.  148.  184.  420.  673.  313.  477.  300.\n",
      "  567.  211.  468.  416.  294.  411.  449.  513.  360.  306.  135.  219.\n",
      "  416.  296.  193.  102.  160.  130.  292.  230.   89.  241.  357.  287.\n",
      "  513.  377.  181.  184.  354.  335.  183.  225.  182.  176.  236.  120.\n",
      "   38.   92.  138.  174.  145.  156.  104.  129.  206.  185.  152.  375.\n",
      "  324.  277.  244.  327.   82.  279.   40.  127.   75.   72.  118.  345.\n",
      "  306.  240.  312.  356.  415.  303.  609.  366.  134.  332.  361.  518.\n",
      "  499.  387.  318.  331. 1029.  322.  156.  195.  130.  148.  230.  248.\n",
      "   42.  222.  178.  570.  240.  323.  190.  319.  295.  339.  336.  183.\n",
      "  185.  226.  229.  174.   52.  174.  191.  191.  198.  184.  158.  142.\n",
      "  192.  239.  209.  327.  210.  237.  152.   52.  184.   44.  133.  148.\n",
      "  175.  196.  201.  133.  233.  134.  299.  186.   77.  218.  203.  169.\n",
      "  179.  156.  134.  122.  222.  173.  123.  119.   92.  157.  166.  113.\n",
      "   37.  215.  193.  165.  151.  240.  337.  288.  178.  297.  152.  483.\n",
      "  402.  130.  394.  356.  388.  384.  303.  155.  323.  346.  322.  256.\n",
      "  350.  465.  356.  518.  469.  188.  514.   65.  146.  130.   91.  194.\n",
      "  442.  548.  375.   59.   47.   75.   35.   94.  150.   70.  118.   48.\n",
      "  122.  104.   83.   36.   82.   86.   72.  174.  147.  194.  150.  402.\n",
      "  486.  121.  466.  335.  396.  397.  500.  180.  320.  321.  349.  462.\n",
      "  206.  318.  238.  378.  244.  152.  276.  396.  408.  446.  315.  174.\n",
      "  178.  380.  293.  484.  744.  568.  591.  498.  167.  499.  135.  292.\n",
      "  399.  562.  394.  372.  270.  506.  260.  632.  500.  225.  558.  456.\n",
      "  470.  577.  359.  198.  289.  519.  360.  279.  215.  187.  311.  371.\n",
      "  357.  108.  432.  484.  658.  601. 1018.  212.  280.  491.  433.  254.\n",
      "  128.  205.  178.  254.  201.   69.  208.  186.  204.  158.  181.  129.\n",
      "  166.  212.  313.  274.  274.  318.  280.  371.  458.  120.  446.   52.\n",
      "   86.   59.   43.  196.  422.  408.  344.  267.  156.  325.  181.  483.\n",
      "  478.  191.  468.  243.  371.  302.  267.  217.  243.  445.  342.  259.\n",
      "  186.  235.  286.  383.  454.   80.  493.  300.  341.  292.  374.  345.\n",
      "  397.  416.  711.]\n",
      "0 / 1200\n",
      "100 / 1200\n",
      "200 / 1200\n",
      "300 / 1200\n",
      "400 / 1200\n",
      "500 / 1200\n",
      "600 / 1200\n",
      "700 / 1200\n",
      "800 / 1200\n",
      "900 / 1200\n",
      "1000 / 1200\n",
      "1100 / 1200\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -4.8335e-02 -1.2007e+02  3e+03  1e+01  2e-15\n",
      " 1: -4.5232e-02 -9.9502e+01  1e+02  1e-01  1e-15\n",
      " 2:  4.1042e-03 -2.3646e+00  3e+00  2e-03  1e-15\n",
      " 3: -4.0925e-02 -1.1839e-01  8e-02  2e-05  1e-15\n",
      " 4: -4.8724e-02 -5.3003e-02  4e-03  1e-07  1e-15\n",
      " 5: -4.8817e-02 -4.9049e-02  2e-04  7e-09  9e-16\n",
      " 6: -4.8819e-02 -4.8824e-02  6e-06  2e-10  9e-16\n",
      " 7: -4.8819e-02 -4.8819e-02  6e-08  2e-12  9e-16\n",
      "Optimal solution found.\n",
      "0.14963201189820943\n",
      "Predicting\n",
      "Training and prediction time is:  44.28122615814209\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5952083333333333"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating features\n",
    "x_trainf,x_testf = create_kmers(xtrain, ytrain, xtest, k=5)\n",
    "\n",
    "#splitting for testing\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(x_trainf, ytrain, test_size=0.8)\n",
    "\n",
    "#Initialization\n",
    "init = SVM(polynomial_kernel, 0.1)\n",
    "#SVM Fitting\n",
    "start = time()\n",
    "init.fit_svm(X_train2, y_train2)\n",
    "#Prediction\n",
    "print(\"Predicting\")\n",
    "pred1 = init.predict(X_test2)\n",
    "end = time()\n",
    "print(\"Training and prediction time is: \", end - start)\n",
    "#Accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test2, pred1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving Prediction to file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "col2 = pred1\n",
    "col2[col2 == -1] = 0\n",
    "col2 = col2.astype(int)\n",
    "col1 = range(0, col2.shape[0])\n",
    "predictions = pd.DataFrame({'Id': col1, 'Bound': col2})\n",
    "predictions.to_csv('submission_Test.csv', sep=',', encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
